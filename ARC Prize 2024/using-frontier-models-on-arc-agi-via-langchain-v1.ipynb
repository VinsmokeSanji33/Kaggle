{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":67357,"databundleVersionId":8951125,"sourceType":"competition"}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Using Frontier Models on ARC-AGI via LangChain\n\n[_Video tutorial for this notebook_](https://www.youtube.com/watch?v=crhrzhVjWog)\n\nIn this notebook we'll walk through a basic \"hello world\" example of using an LLM on [ARC-AGI tasks](https://arcprize.org/). This notebook is for demonstration purposes only. We encourage you to evolve it with your own ideas!\n\nWhile notebooks like these are not eligible for [ARC Prize 2024](https://arcprize.org/competition), they can be used to make a submission to [ARC-AGI-Pub](https://arcprize.org/arc-agi-pub), a separate leaderboard which allows API calls. For more information on ARC-AGI-Pub visit the [leaderboard](https://arcprize.org/leaderboard).\n\nFeel free to reach out to the ARC Prize team on [X](https://twitter.com/arcprize), [YouTube](https://www.youtube.com/channel/UC_rdrp-QkrZn-ce9uCE-0EA), [Discord](https://discord.gg/9b77dPAmcA) or team@arcprize.org for questions!\n\nLet's get started!\n\n**Goal**: Create a `submission.json` file with our predictions for each challenge. Even though we are not submitting to Kaggle we will use this file to score ourselves against the public task sets.\n\nWe are starting with a new notebook instance, so we'll first have to install our packages. We'll use [LangChain](https://python.langchain.com/v0.1/docs/get_started/introduction), an open source library for LLM orchestration, for their ability to help with swapping models, prompt template and output parsing.\n\n_Run each cell one by one by clicking the 'play' button in the top left of each cell or shift-enter with a cell highlighted_","metadata":{}},{"cell_type":"code","source":"!pip install -qU langchain 2>/dev/null\n!pip install -qU langchain-openai 2>/dev/null\n!pip install -qU langchain-anthropic 2>/dev/null\n!pip install -qU langchain-google-genai 2>/dev/null","metadata":{"execution":{"iopub.status.busy":"2024-06-25T20:44:07.840278Z","iopub.execute_input":"2024-06-25T20:44:07.840721Z","iopub.status.idle":"2024-06-25T20:45:27.111803Z","shell.execute_reply.started":"2024-06-25T20:44:07.840671Z","shell.execute_reply":"2024-06-25T20:45:27.109938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next we'll import our packages. Notice the `from kaggle_secrets import UserSecretsClient` import. This is how Kaggle manages API keys and secrets. See this [article](https://www.kaggle.com/discussions/product-feedback/114053) for more information.","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient # Used to manage secrets. Similar to .env\n\nimport langchain # Main LangChain import\nfrom langchain_openai import ChatOpenAI # To work with OpenAI\nfrom langchain_anthropic import ChatAnthropic # To work with Anthropic (optional)\nfrom langchain_google_genai import ChatGoogleGenerativeAI # To work with Gemini (optional)\nfrom langchain_core.output_parsers import JsonOutputParser # To help with structured output\nfrom langchain_core.prompts import PromptTemplate # To help create our prompt\nfrom langchain_core.pydantic_v1 import BaseModel, Field # To help with defining what output structure we want\n\nfrom typing import List, Tuple\nimport os\nimport json","metadata":{"execution":{"iopub.status.busy":"2024-06-25T20:45:27.11486Z","iopub.execute_input":"2024-06-25T20:45:27.115344Z","iopub.status.idle":"2024-06-25T20:45:28.3774Z","shell.execute_reply.started":"2024-06-25T20:45:27.115294Z","shell.execute_reply":"2024-06-25T20:45:28.37616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ARC-AGI Data\n\nNext let's take a look at the files in our environment, this will help us navigate them later","metadata":{}},{"cell_type":"code","source":"print (\"Files included\")\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2024-06-25T20:45:30.435594Z","iopub.execute_input":"2024-06-25T20:45:30.436214Z","iopub.status.idle":"2024-06-25T20:45:30.45632Z","shell.execute_reply.started":"2024-06-25T20:45:30.436179Z","shell.execute_reply":"2024-06-25T20:45:30.455016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see we have a few different `challenge` and `solution` files. The files held in `kaggle/input/arc-prize-2024` are hosted by the official competition. For more information on these files check out the [competition data overview](https://www.kaggle.com/competitions/arc-prize-2024/data) or [ARC Prize Guide](https://arcprize.org/guide#data-structure).\n\nThen let's set up a quick dictionary that will allow us to swap sets of `challenges` and `solutions` quickly.\n* `training` : This is a set of 400 public training tasks from the official [ARC-AGI repo](https://github.com/fchollet/ARC-AGI)\n* `evaluation` : This is a set of 400 evaluation tasks from the official [ARC-AGI repo](https://github.com/fchollet/ARC-AGI)\n\n* `_challenges` : Contains a series of `train` input and output pairs along with a `test` input\n* `_solutions` : Contains the output to the `test` input. This is what you're model will try and predict.\n\nYou are not limited to testing with these task sets (though they are very convenient to use!), you could make your own tasks that follow the same format to test.\n\nIf you ever want to see one of these pubic tasks, head over to [ARCprize.org/play](https://arcprize.org/play?task=00576224) along with your `task_id` (ex: https://arcprize.org/play?task=00576224).","metadata":{}},{"cell_type":"code","source":"task_sets = {\n    'training' : {\n        'challenges' : '/kaggle/input/arc-prize-2024/arc-agi_training_challenges.json',\n        'solutions' : '/kaggle/input/arc-prize-2024/arc-agi_training_solutions.json',\n    },\n    'evaluation' : {\n        'challenges' : '/kaggle/input/arc-prize-2024/arc-agi_evaluation_challenges.json',\n        'solutions' : '/kaggle/input/arc-prize-2024/arc-agi_evaluation_solutions.json',\n    }\n}","metadata":{"execution":{"iopub.status.busy":"2024-06-25T20:45:33.660984Z","iopub.execute_input":"2024-06-25T20:45:33.661399Z","iopub.status.idle":"2024-06-25T20:45:33.667874Z","shell.execute_reply.started":"2024-06-25T20:45:33.661367Z","shell.execute_reply":"2024-06-25T20:45:33.666499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then let's create a function that will load up our challenges and tasks according to the task set we choose","metadata":{"execution":{"iopub.status.busy":"2024-06-20T19:23:37.16304Z","iopub.execute_input":"2024-06-20T19:23:37.163428Z","iopub.status.idle":"2024-06-20T19:23:37.171075Z","shell.execute_reply.started":"2024-06-20T19:23:37.163398Z","shell.execute_reply":"2024-06-20T19:23:37.169385Z"}}},{"cell_type":"code","source":"def load_tasks_from_file(task_set):\n    \"\"\"\n    Loads the tasks from the file and returns the challenges and solutions tasks\n    \"\"\"\n    with open(task_set['challenges'], \"r\") as tasks:\n        challenges = json.load(tasks)\n\n    with open(task_set['solutions'], \"r\") as tasks:\n        solutions = json.load(tasks)\n\n    return challenges, solutions","metadata":{"execution":{"iopub.status.busy":"2024-06-25T20:45:34.162166Z","iopub.execute_input":"2024-06-25T20:45:34.162573Z","iopub.status.idle":"2024-06-25T20:45:34.169827Z","shell.execute_reply.started":"2024-06-25T20:45:34.162537Z","shell.execute_reply":"2024-06-25T20:45:34.168526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's look at a quick example of a task challenge","metadata":{}},{"cell_type":"code","source":"challenges, solutions = load_tasks_from_file(task_set=task_sets['training'])\nchallenges['0520fde7']","metadata":{"execution":{"iopub.status.busy":"2024-06-25T20:45:34.922073Z","iopub.execute_input":"2024-06-25T20:45:34.922451Z","iopub.status.idle":"2024-06-25T20:45:35.048531Z","shell.execute_reply.started":"2024-06-25T20:45:34.922423Z","shell.execute_reply":"2024-06-25T20:45:35.047414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can see our `train` input output pairs (there are 3 pairs) and our `test` inputs. You can view this task at https://arcprize.org/play?task=0520fde7\n\nIn this example there are 3 test inputs. Your model must make a prediction for each of them. Your score will be the sum of your scores on each input (0 or 1) divided by the number of inputs. If you got 1 of 3 correct, you'd get a score of 33% for the task.\n\nThis is a special example, most tasks (~96%) have only one test input.\n\n## LLM Set up\n\nWe'll use [LangChain](https://www.langchain.com/) for our LLM orchestration. This will allow us to use their out of the box output parsing, model selection and prompt templates. You can also use [LangSmith](https://www.langchain.com/langsmith) (free tier) to observe your output but that is outside the scope of this template.\n\nFirst let's get our model ready. I'll be using `gpt-4o` to start. But you can swap whatever model you'd like! See more models [here](https://python.langchain.com/v0.2/docs/integrations/chat/). We set `max_tokens=3000` because the default token limit may not capture the full output of a prediction.\n\nMake sure that your api key secret name below matches what you put in your Kaggle secrets.","metadata":{}},{"cell_type":"code","source":"# llm = ChatOpenAI(model='gpt-4o', openai_api_key=UserSecretsClient().get_secret('OPENAI_API_KEY'), max_tokens=3000)\n\n## And incase you want to try Anthropic\n# llm = ChatAnthropic(model='claude-3-5-sonnet-20240620', api_key=UserSecretsClient().get_secret(\"ANTHROPIC_API_KEY\"), max_tokens=3000)\nllm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", google_api_key=UserSecretsClient().get_secret(\"GOOGLE_API_KEY\"), max_tokens=3000)","metadata":{"execution":{"iopub.status.busy":"2024-06-25T20:46:06.103338Z","iopub.execute_input":"2024-06-25T20:46:06.103798Z","iopub.status.idle":"2024-06-25T20:46:06.285102Z","shell.execute_reply.started":"2024-06-25T20:46:06.103763Z","shell.execute_reply":"2024-06-25T20:46:06.283713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #e6f3ff; padding: 10px; border-radius: 5px;\"><strong>Optimization point:</strong> Experiment with different base or fine-tuned models</div><br>\n\nLLMs can not ingest a json object, so first order of business is to convert the `train` pairs and `test` input into a string (which LLMs love).\n\nThis is a highly creative process and the below is only a starting point. There is much more prompt engineering you can do to the below that will likely improve your scores.\n\nDo not take the format below as the way you *should* do it, but rather as one example. Have fun and share what works for you!\n\nThe output of the `json_task_to_string` (below) will be used in the prompt we give to the LLM.","metadata":{}},{"cell_type":"code","source":"def json_task_to_string(challenge_tasks: dict, task_id: str, test_input_index: int) -> str:\n    \"\"\"\n    challenge_tasks: dict a list of tasks\n    task_id: str the id of the task we want to convert to a string\n    \n    Convert your json task into a string so you can pass it to your LLM.\n    This is a crucial step where you can use your creativity to edit how tasks are represented.\n    \"\"\"\n    json_task = challenge_tasks[task_id]\n\n    final_output = \"\"\n\n    train_tasks = json_task['train']\n    test_task = json_task['test']\n\n    final_output = \"Training Examples\\n\"\n\n    for i, task in enumerate(train_tasks):\n        final_output += f\"Example {i + 1}: Input\\n[\"\n        for row in task['input']:\n            final_output += f\"\\n{str(row)},\"\n\n        final_output += \"]\\n\\n\"\n        final_output += f\"Example {i + 1}: Output\\n[\"\n\n        for row in task['output']:\n            final_output += f\"\\n{str(row)},\"\n\n        final_output += \"]\\n\\n\"\n\n    final_output += \"Test\\n[\"\n    for row in test_task[test_input_index]['input']:\n        final_output += f\"\\n{str(row)}\"\n\n    final_output += \"]\\n\\nYour Response:\"\n\n    return final_output","metadata":{"execution":{"iopub.status.busy":"2024-06-25T20:46:08.699882Z","iopub.execute_input":"2024-06-25T20:46:08.70026Z","iopub.status.idle":"2024-06-25T20:46:08.709818Z","shell.execute_reply.started":"2024-06-25T20:46:08.700233Z","shell.execute_reply":"2024-06-25T20:46:08.708387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #e6f3ff; padding: 10px; border-radius: 5px;\"><strong>Optimization point: </strong>Try representing tasks in other ways. Perhapse with letters instead of ints, xml, etc.</div><br>\n\nLet's look at an example of this using the task we had before","metadata":{}},{"cell_type":"code","source":"task_string = json_task_to_string(challenges, '0520fde7', 0)\nprint (task_string)","metadata":{"execution":{"iopub.status.busy":"2024-06-25T20:46:22.858432Z","iopub.execute_input":"2024-06-25T20:46:22.858955Z","iopub.status.idle":"2024-06-25T20:46:22.864876Z","shell.execute_reply.started":"2024-06-25T20:46:22.85892Z","shell.execute_reply":"2024-06-25T20:46:22.863668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Output Parsing\n\nAwesome! Now we have a string we can work with. But what about the output from the LLM?\n\nLLMs aren't *great* at outputting valid json, so we'll take any help we can get.\n\nLangChain has a [few ways](https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/) to do output parsing (ensuring the output is in the format you'd like). We'll use a [JsonOutputParser](https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/types/json/) for our use case. Feel free to use any other you'd like.\n\nTo do this we need a data structure, we'll have a simple prediction that is a list of lists. This won't be 100% accurate all the time (there are better ways to ensure data structures (like [instructor](https://github.com/jxnl/instructor)), but we'll add some retries later in case it fails).","metadata":{}},{"cell_type":"code","source":"# Defining a prediction as a list of lists\nclass ARCPrediction(BaseModel):\n    prediction: List[List] = Field(..., description=\"A prediction for a task\")","metadata":{"execution":{"iopub.status.busy":"2024-06-25T20:46:25.474251Z","iopub.execute_input":"2024-06-25T20:46:25.474685Z","iopub.status.idle":"2024-06-25T20:46:25.481914Z","shell.execute_reply.started":"2024-06-25T20:46:25.474648Z","shell.execute_reply":"2024-06-25T20:46:25.480696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Language Model Prediction\nNow that we have our data structure let's move to the LLM prompt and call to get our prediction.\n\nThere will be 3 main pieces to this LLM call\n* Model: This is the LLM that we'll use\n* Prompt: The prompt that we'll send to the LLM. Because this is a 'hello world' example, we'll use an extremely simple prompt. You should edit this with different ideas you have about how to score better on ARC-AGI.\n* Parser: The output parser that we'll use to ensure our output is in the correct format\n\nWe'll wrap these up with [LangChain Expression Language](https://python.langchain.com/v0.1/docs/expression_language/) to keep it simple.","metadata":{}},{"cell_type":"code","source":"def get_task_prediction(challenge_tasks, task_id, test_input_index) -> List[List]:\n    \"\"\"\n    challenge_tasks: dict a list of tasks\n    task_id: str the id of the task we want to get a prediction for\n    test_input_index: the index of your test input. 96% of tests only have 1 input.\n\n    Given a task, predict the test output\n    \"\"\"\n\n    # Get the string representation of your task\n    task_string = json_task_to_string(challenge_tasks, task_id, test_input_index)\n    \n    # Set up a parser to inject instructions into the prompt template.\n    parser = JsonOutputParser(pydantic_object=ARCPrediction)\n\n    # Create your prompt template. This is very rudimentary! You should edit this to do much better.\n    # For example, we don't tell the model what it's first attempt was (so it can do a different one), that might help!\n    prompt = PromptTemplate(\n        template=\"You are a bot that is very good at solving puzzles. Below is a list of input and output pairs with a pattern.\" \n                    \"Identify the pattern, then apply that pattern to the test input to give a final output\"\n                    \"Just give valid json list of lists response back, nothing else. Do not explain your thoughts.\"\n                    \"{format_instructions}\\n{task_string}\\n\",\n        input_variables=[\"task_string\"],\n        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n    )\n\n    # Wrap up your chain with LCEL\n    chain = prompt | llm | parser\n\n    # Optional, print out the prompt if you want to see it. If you use LangSmith you could view this there as well.\n    # print (f\"Prompt:\\n\\n{prompt.format(task_string=task_string)}\")\n    \n    # Finally, go get your prediction from your LLM. Ths will make the API call.\n    output = chain.invoke({\"task_string\": task_string})\n\n    # Because the output is structured, get the prediction key. If it isn't there, then just get the output\n    if isinstance(output, dict):\n        prediction = output.get('prediction', output)\n    else:\n        prediction = output\n\n    # Safety measure to error out if you don't get a list of lists of ints back. This will spark a retry later.\n    if not all(isinstance(sublist, list) and all(isinstance(item, int) for item in sublist) for sublist in prediction):\n        print(\"Warning: Output must be a list of lists of integers.\")\n        print (f\"Errored Output: {prediction}\")\n        raise ValueError(\"Output must be a list of lists of integers.\")\n    \n    # Let's find the shape of our prediction\n    num_rows = len(prediction)\n    num_cols = len(prediction[0]) if num_rows > 0 else 0\n    print(f\"    Prediction Grid Size: {num_rows}x{num_cols}\\n\")\n    \n    return prediction","metadata":{"execution":{"iopub.status.busy":"2024-06-25T20:46:26.247144Z","iopub.execute_input":"2024-06-25T20:46:26.247564Z","iopub.status.idle":"2024-06-25T20:46:26.2613Z","shell.execute_reply.started":"2024-06-25T20:46:26.247529Z","shell.execute_reply":"2024-06-25T20:46:26.260049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #e6f3ff; padding: 10px; border-radius: 5px;\"><strong>Optimization point:</strong> Try prompt engineering your way to a better score</div><br>\n\n<div style=\"background-color: #e6f3ff; padding: 10px; border-radius: 5px;\"><strong>Optimization point:</strong> Let the model know about the 1st attempt that was made so you're able to product a 2nd one</div><br>\n\nGreat! Now that we have the pieces to make our prediction we can run through our challenges and start predicting them.\n\nLet's make a function that will run through challenges and then output a list of submissions for us.\n\nNote: This list of submissions needs to be in a specific format. We'll later use this to populate a `submission.json` file that matches the format used for the official Kaggle competition. See more information on that [here](www.kaggle.com/competitions/arc-prize-2024/overview/evaluation) or view an example of `submission.json` file [here](https://www.kaggle.com/competitions/arc-prize-2024/data?select=sample_submission.json).\n\nNote: For ARC Prize 2024, you get 2 attempts at each task input. Both attempts must be submitted at the same time. If either of them are correct you get a full score of 1.","metadata":{}},{"cell_type":"code","source":"def run_model(challenges, NUM_ATTEMPTS=2, RETRY_ATTEMPTS=3, NUM_TASKS=None):\n    \"\"\"\n    challenges: dict a list of challenges. This should come directly from your _challenges file\n    NUM_ATTEMPTS: int the number of times to attempt a prediction. The official competition has 2 attempts.\n    RETRY_ATTEMPTS: int the number of times to retry a prediction if it fails\n    NUM_TASKS: int, If set, this represents the the number of tasks you'd like to test. If None then the all challeneges will be tested\n\n    Loop through your challenges and produce a submission.json file you can submit for a score.\n    \"\"\"\n\n    # A dict to hold your submissions that you'll return after all predictions are made\n    submission = {}\n\n    # Run through each task in your challenge set\n    for i, task_id in enumerate(challenges):\n        task_attempts = []  # List to store all attempts for the current task\n\n        # Go through each test pair to get a prediction. 96% of challenges have 1 pair.\n        for t, pair in enumerate(challenges[task_id]['test']):\n            print(f\"Starting task #{i + 1} ({task_id}), pair #{t+1}\")\n\n            # Dictionary to store attempts for the current test pair\n            pair_attempts = {}  \n\n            # Run through each prediction attempt\n            for attempt in range(1, NUM_ATTEMPTS + 1):\n                attempt_key = f\"attempt_{attempt}\"\n                pair_attempts[attempt_key] = [] # Init your attempt\n\n                # Try to get a prediction, with retries in case of failure\n                for retry in range(RETRY_ATTEMPTS):\n                    try:\n                        print(f\"    Predicting attempt #{attempt}, retry #{retry + 1}\")\n                        prediction = get_task_prediction(challenge_tasks=challenges,\n                                                         task_id=task_id,\n                                                         test_input_index=t)\n                        \n                        # If you get a valid prediction (list of lists of ints) with no error, then log the attempt\n                        pair_attempts[attempt_key] = prediction\n                        break  # Break the retry loop if prediction is successful\n                    except Exception as e:\n                        print(f\"Retrying: {e}\")\n                        if retry == RETRY_ATTEMPTS - 1:\n                            pair_attempts[attempt_key] = []  # Assign None if all retries fail\n\n            # After you get your attempts, append them to the task attempts\n            task_attempts.append(pair_attempts)\n\n        # Append the task attempts to the submission with the task_id as the key\n        submission[task_id] = task_attempts\n\n        # If you want to stop after N tasks, uncomment the below\n        if NUM_TASKS is not None and i + 1 == NUM_TASKS:\n            break\n\n    return submission","metadata":{"execution":{"iopub.status.busy":"2024-06-25T20:46:27.576666Z","iopub.execute_input":"2024-06-25T20:46:27.577071Z","iopub.status.idle":"2024-06-25T20:46:27.589449Z","shell.execute_reply.started":"2024-06-25T20:46:27.577038Z","shell.execute_reply":"2024-06-25T20:46:27.588159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wow! Now we have a way to get a prediction for each challenge. Let's try this out with one example from the `training` tasks","metadata":{"execution":{"iopub.status.busy":"2024-06-20T19:28:12.229731Z","iopub.execute_input":"2024-06-20T19:28:12.230825Z","iopub.status.idle":"2024-06-20T19:28:12.238693Z","shell.execute_reply.started":"2024-06-20T19:28:12.230782Z","shell.execute_reply":"2024-06-20T19:28:12.23726Z"}}},{"cell_type":"code","source":"# Load up training tasks\nchallenges, solutions = load_tasks_from_file(task_set=task_sets['training'])\n\n# Run the model on a single task\nsubmission = run_model(challenges, NUM_TASKS=1)\n\n# Print the submission\nprint (submission)","metadata":{"execution":{"iopub.status.busy":"2024-06-25T20:47:29.962579Z","iopub.execute_input":"2024-06-25T20:47:29.963693Z","iopub.status.idle":"2024-06-25T20:47:51.441064Z","shell.execute_reply.started":"2024-06-25T20:47:29.963595Z","shell.execute_reply":"2024-06-25T20:47:51.439848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Awesome! That is great. Let's break down what we see:\n* `{'007bbfb7':`: This is our key that will let us know which task the solution is for\n* `[{'attempt_1':` This list will hold our attempt dicts. There will be one dict per test input (96% of the time there will only be one). This is also our first attempt\n*  `'attempt_2': `: Our second attempt\n\nLet's create a quick function that will take our submission output and save it as a `submission.json` file.","metadata":{}},{"cell_type":"code","source":"def create_submission_file(submission, file_name='submission.json'):\n    \"\"\"\n    Save a submission file to the specified file name\n    \"\"\"\n    with open(file_name, \"w\") as file:\n        json.dump(submission, file)\n\n    print (f\"Submission saved to {file_name}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-25T20:47:56.862324Z","iopub.execute_input":"2024-06-25T20:47:56.862782Z","iopub.status.idle":"2024-06-25T20:47:56.86935Z","shell.execute_reply.started":"2024-06-25T20:47:56.862745Z","shell.execute_reply":"2024-06-25T20:47:56.86798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lastly, we want a way to score our submissions. We'll do this by comparing `submission.json` to the `_solutions` file for the corresponding challenge.\n\nThis function will walk down the submission dict, look at each task, find the corresponding solution and then compare the two. The extra code is to account for multiple attempts and multiple test inputs. See more on scoring [here](https://www.kaggle.com/code/gregkamradt/arc-prize-scoring).","metadata":{}},{"cell_type":"code","source":"def score_submission(submission_file_name, solutions) -> Tuple[float, int]:\n    \"\"\"\n    submission_file_name: str, the file name of your submission file\n    solutions: dict, the ground truth solutions you'd like to test against\n    \n    Read a submission from file, score it, then return the score\n    \"\"\"\n    print (f\"Scoring {submission_file_name}\\n\")\n\n    # Open your submission file\n    with open(submission_file_name, \"r\") as file:\n        submission = json.load(file)\n\n    total_score = 0\n    total_tasks = 0\n\n    # Loop through each task in your submission to grade it\n    for task_id, task_submission in submission.items():\n        total_tasks += 1\n        task_score = 0\n        num_pairs = len(task_submission)\n\n        # Go through each task. Most will only have 1\n        for pair_index, pair_attempts in enumerate(task_submission):\n            print(f\"Scoring Task {task_id} pair #{pair_index+1}\")\n            pair_correct = False\n\n            # Look at both of your attempts\n            for attempt_key, attempt in pair_attempts.items():\n                \n                # check to see if one is correct\n                if attempt == solutions[task_id][pair_index]:\n                    print(f\"Task Id {task_id} pair {pair_index+1} {attempt_key} matches solution\")\n                    pair_correct = True\n                    break # If it is correct, log it and break the loop\n\n            if pair_correct:\n                task_score += 1\n\n        task_score /= num_pairs\n        total_score += task_score\n\n    return {\n        'total_score': total_score,\n        'total_tasks_scored': total_tasks\n    }","metadata":{"execution":{"iopub.status.busy":"2024-06-25T20:47:57.747818Z","iopub.execute_input":"2024-06-25T20:47:57.748752Z","iopub.status.idle":"2024-06-25T20:47:57.759175Z","shell.execute_reply.started":"2024-06-25T20:47:57.748708Z","shell.execute_reply":"2024-06-25T20:47:57.757984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bring it all together\n\nGreat! Now that we have a way to get a prediction for each challenge and a way to score our submissions, let's put it all together.\n\nThis is a simple function that will load up the tasks, run the model, create a submission file and then score the submission.","metadata":{}},{"cell_type":"code","source":"def main(task_set='training', NUM_TASKS=None, submission_file_name='submission.json'):\n    # Load datasets\n    challenges, solutions = load_tasks_from_file(task_set=task_sets[task_set])\n\n    # # Run the model\n    submission = run_model(challenges, NUM_TASKS=NUM_TASKS)\n\n    # Create (and overwrite) a submission file\n    create_submission_file(submission, file_name=submission_file_name)\n\n    # Score the submission\n    score_result = score_submission(solutions = solutions, submission_file_name=submission_file_name)\n\n    print(f\"Final score: {score_result['total_score']} of {score_result['total_tasks_scored']} ({round(score_result['total_score']/score_result['total_tasks_scored'] * 100, 2)}%)\")","metadata":{"execution":{"iopub.status.busy":"2024-06-25T20:47:58.79302Z","iopub.execute_input":"2024-06-25T20:47:58.793444Z","iopub.status.idle":"2024-06-25T20:47:58.801329Z","shell.execute_reply.started":"2024-06-25T20:47:58.793409Z","shell.execute_reply":"2024-06-25T20:47:58.800019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's run our model!","metadata":{}},{"cell_type":"code","source":"main(task_set='evaluation', NUM_TASKS=2)","metadata":{"execution":{"iopub.status.busy":"2024-06-25T20:48:05.829621Z","iopub.execute_input":"2024-06-25T20:48:05.830049Z","iopub.status.idle":"2024-06-25T20:48:14.928819Z","shell.execute_reply.started":"2024-06-25T20:48:05.830012Z","shell.execute_reply":"2024-06-25T20:48:14.926673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Awesome! Congratulations. You just made a predicted outputs to ARC-AGI tasks using gpt-4o. Remember, with LangChain you can easily swap out different models. How do you think [claude-3.5-sonnet](https://www.anthropic.com/news/claude-3-5-sonnet) or [Gemini](https://gemini.google.com/) would do?\n\nIf you would like to make a ARC-AGI-Pub high-score claim, run your model against `kaggle/input/arc-prize-2024/arc-agi_evaluation_challenges.json` and report your score [here](https://docs.google.com/forms/d/e/1FAIpQLSdyPg16R2BmGb6nZpsTAty4HqI4WjhpZcg951ApjzSfHJ7Kpw/viewform). Serious submissions will be considered to be tested against a semi-private set of challenges. Top scores of ARC-AGI-Pub will be reported on the [ARC-AGI-Pub Leaderboard](https://arcprize.org/leaderboard).\n\nWe love hearing from the community. If you expand on this notebook please share it with us on [Twitter](https://twitter.com/arcprize), [Discord](https://discord.gg/9b77dPAmcA) or at team@arcprize.org.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}